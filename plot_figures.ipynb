{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86b3748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pysindy as ps\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "import time\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from statistics import mean \n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415217d5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def confidenceInterval(alist, ciColumn, trials):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval, mean, and standard deviation for a specified column in a list of datasets.\n",
    "    \n",
    "    Args:\n",
    "    alist (list): List of datasets.\n",
    "    ciColumn (int): Index of the column for confidence interval calculation.\n",
    "    trials (int): Not used in the function body and could be removed or repurposed.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: lower and upper bounds of the confidence interval, mean, and standard deviation.\n",
    "    \"\"\"\n",
    "    mean_1 = mean_of_list(alist, ciColumn)  # Calculate mean for each point\n",
    "    std_1 = std_of_list(alist, ciColumn)  # Calculate standard deviation\n",
    "    confidence_low, confidence_high = bootstrap_confidence_interval_column(alist, ciColumn)  # Get confidence interval\n",
    "    return confidence_low, confidence_high, mean_1, std_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d723cc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval_column(data, ciColumn, num_bootstrap_samples=1000, confidence_level=0.90):\n",
    "    \"\"\"\n",
    "    Generate bootstrap confidence intervals for a specified column across multiple datasets.\n",
    "    \n",
    "    Args:\n",
    "    data (list): List of datasets where each dataset is a list or a numpy array.\n",
    "    ciColumn (int): The index of the column for which to calculate the confidence interval.\n",
    "    num_bootstrap_samples (int): Number of bootstrap samples to generate.\n",
    "    confidence_level (float): Confidence level for the interval.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Lists of lower bounds, upper bounds of the confidence intervals.\n",
    "    \"\"\"\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "    for dataset in data:\n",
    "        column_data = np.array(dataset)[:, ciColumn]\n",
    "        bootstrap_means = [np.mean(np.random.choice(column_data, size=len(column_data), replace=True)) for _ in range(num_bootstrap_samples)]\n",
    "        alpha = (1 - confidence_level) / 2\n",
    "        lower_bounds.append(np.percentile(bootstrap_means, 100 * alpha))\n",
    "        upper_bounds.append(np.percentile(bootstrap_means, 100 * (1 - alpha)))\n",
    "    return lower_bounds, upper_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d328ab",
   "metadata": {
    "code_folding": [
     0,
     17
    ]
   },
   "outputs": [],
   "source": [
    "def replace_column(matrix, new_column, snr_index, column_index):\n",
    "    \"\"\"\n",
    "    Replace a column in a matrix with a new column vector.\n",
    "    \n",
    "    Args:\n",
    "        matrix (list of lists): The original matrix.\n",
    "        new_column (list): The new column vector to replace the existing column.\n",
    "        snr_index (int): Index for the sub-list in matrix where the replacement should occur.\n",
    "        column_index (int): Index of the column to be replaced.\n",
    "    \n",
    "    Returns:\n",
    "        list of lists: The matrix with the updated column.\n",
    "    \"\"\"\n",
    "    if column_index < 0 or column_index >= len(matrix[0]):\n",
    "        raise ValueError(\"Invalid column_index\")\n",
    "    if len(new_column) != len(matrix[snr_index][column_index]):\n",
    "        raise ValueError(\"New column length must match the matrix height\")\n",
    "    for i in range(len(matrix[snr_index][column_index])):\n",
    "        matrix[snr_index][column_index][i] = new_column[i]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08e2a57",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def flatten(data, column, column_sin):\n",
    "    \"\"\"\n",
    "    Flatten a specific column from a list of datasets. This function is intended to extract and flatten data\n",
    "    from a structured dataset where data points are organized in columns.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of datasets where each dataset is potentially a list of lists.\n",
    "        column (int): Index of the column to extract and flatten.\n",
    "        column_sin (int): Unused in this function but might be intended for future use to specify a second column.\n",
    "    \n",
    "    Returns:\n",
    "        list: A flat list of values extracted from the specified column across all datasets.\n",
    "    \"\"\"\n",
    "    flattened_data = []\n",
    "    for i in range(len(data)):\n",
    "        element = data[i][column]  # Extract the column for flattening\n",
    "        flattened_data.append(element)\n",
    "    flat_list = [item for sublist in flattened_data for item in sublist]  # Flatten the list of lists\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6c2869",
   "metadata": {
    "code_folding": [
     0,
     17
    ]
   },
   "outputs": [],
   "source": [
    "def normalise_decision_time(data,column_dt,column_dt_sindy,lcaddm=False):\n",
    "    \"\"\"\n",
    "    Normalize decision times in the data according to the maximum and minimum values found across all data.\n",
    "    \n",
    "    Args:\n",
    "    data (list): Data containing decision times.\n",
    "    column_dt (int): Index of the decision time column in each sub-list.\n",
    "    column_dt_sindy (int): Index of the decision time column in the SINDy model.\n",
    "    lcaddm (bool): Flag to indicate special processing condition.\n",
    "    \n",
    "    Returns:\n",
    "    list: Data with normalized decision times.\n",
    "    \"\"\"\n",
    "    model_dt_flat=flatten(data,column_dt,column_dt_sindy)\n",
    "    max_dt=max(model_dt_flat)\n",
    "    min_dt=min(model_dt_flat)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        normalised=[]\n",
    "        normalised_sindy=[]\n",
    "        for c in range(len(data[i][column_dt])):\n",
    "            if i==4 and c==9999 and lcaddm==True:\n",
    "                norm_model_times = (mean(data[i][column_dt])- min_dt) / (max_dt - min_dt)\n",
    "                norm_model_times_sindy = (mean(data[i][column_dt_sindy]) - min_dt) / (max_dt - min_dt)\n",
    "                normalised.append(norm_model_times)\n",
    "                normalised_sindy.append(norm_model_times_sindy)\n",
    "            else:\n",
    "                norm_model_times = (data[i][column_dt][c] - min_dt) / (max_dt - min_dt)\n",
    "                norm_model_times_sindy = (data[i][column_dt_sindy][c] - min_dt) / (max_dt - min_dt)\n",
    "                normalised.append(norm_model_times)\n",
    "                normalised_sindy.append(norm_model_times_sindy)\n",
    "        #print(len(normalised))\n",
    "        data=replace_column(data, normalised, i, column_dt)\n",
    "        data=replace_column(data, normalised_sindy, i, column_dt_sindy)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "909f6e5f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def cliffs_delta(x, y):\n",
    "    \"\"\"\n",
    "    Compute Cliff's Delta, a measure of effect size, comparing two samples.\n",
    "    \n",
    "    Args:\n",
    "    x (array): First sample.\n",
    "    y (array): Second sample.\n",
    "    \n",
    "    Returns:\n",
    "    float: Cliff's Delta value.\n",
    "    \"\"\"\n",
    "    all_values = np.concatenate((x, y))\n",
    "    ranks = np.argsort(all_values)\n",
    "    rx = np.mean(ranks[:len(x)])\n",
    "    ry = np.mean(ranks[len(x):])\n",
    "    delta = (rx - ry) / len(all_values)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d44dd5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ks_statistical_analysis(data, column, column_sin, snr, no_column=False, data_sindy=None):\n",
    "    \"\"\"\n",
    "    Perform the Kolmogorov-Smirnov test and compute Cliff's Delta to assess the differences\n",
    "    between distributions in the provided datasets.\n",
    "    \n",
    "    Args:\n",
    "        data (list): A list of datasets.\n",
    "        column (int): Index of the primary column for comparison.\n",
    "        column_sin (int): Index of the secondary column for comparison.\n",
    "        snr (list): Signal-to-noise ratios or identifiers for the datasets.\n",
    "        no_column (bool): Indicates if the data lacks structured columns.\n",
    "        data_sindy (list, optional): Secondary dataset list for comparison.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Contains D-statistics, p-values, significant points based on p-value,\n",
    "               and effect sizes (Cliff's Delta).\n",
    "    \"\"\"\n",
    "    p = np.empty(len(data))\n",
    "    d_stat = np.empty(len(data))  # KS statistic\n",
    "    effect_size = np.empty(len(data))  # cliff's d\n",
    "\n",
    "    if no_column:\n",
    "        for i in range(len(data)):\n",
    "            group1 = data[i]\n",
    "            group2 = data_sindy[i]\n",
    "            d_stat[i], p[i] = ks_2samp(group1, group2)\n",
    "            effect_size[i] = cliffs_delta(group1, group2)\n",
    "    else:\n",
    "        for i in range(len(data)):\n",
    "            group1 = data[i][column]\n",
    "            group2 = data[i][column_sin]\n",
    "            d_stat[i], p[i] = ks_2samp(group1, group2)\n",
    "            effect_size[i] = cliffs_delta(group1, group2)\n",
    "\n",
    "    significant_points = [snr[i] if p[i] < .05 else None for i in range(len(p))]\n",
    "    return d_stat, p, significant_points, effect_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebca6edd",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def mean_of_list(alist, column, no_column=False):\n",
    "    \"\"\"\n",
    "    Calculate the mean of a specified column in a list of datasets, or of each dataset if no_column is True.\n",
    "    \n",
    "    Args:\n",
    "        alist (list): List of datasets, where each dataset can be a list of lists or a list of numbers.\n",
    "        column (int): Index of the column to compute the mean for.\n",
    "        no_column (bool): If True, computes the mean of the entire dataset (assumed to be flat).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of means for each dataset.\n",
    "    \"\"\"\n",
    "    if no_column:\n",
    "        return [mean(dataset) for dataset in alist]\n",
    "    else:\n",
    "        return [mean(dataset[column]) for dataset in alist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e62df32",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def std_of_list(alist, column, no_column=False):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation of a specified column in a list of datasets, or of each dataset if no_column is True.\n",
    "    \n",
    "    Args:\n",
    "        alist (list): List of datasets.\n",
    "        column (int): Index of the column to compute the standard deviation for.\n",
    "        no_column (bool): If True, computes the standard deviation of the entire dataset (assumed to be flat).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of standard deviations for each dataset.\n",
    "    \"\"\"\n",
    "    if no_column:\n",
    "        return [std(dataset) for dataset in alist]\n",
    "    else:\n",
    "        return [std(dataset[column]) for dataset in alist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb561fe0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def average_activity_multidimensional(data, trial_threshold=100):\n",
    "    \"\"\"\n",
    "    Calculate the average activity across multiple populations in multidimensional data, truncated to the last valid\n",
    "    time point based on a minimum trial threshold.\n",
    "\n",
    "    Args:\n",
    "        data (list of list of lists): Data containing multiple populations over multiple samples, structured as a list\n",
    "                                      of samples, where each sample contains lists of population activity data.\n",
    "        trial_threshold (int): The minimum number of valid trials required for a time point to be considered in the average.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of averages for each population, truncated to the minimum last valid time point across populations.\n",
    "    \"\"\"\n",
    "    # Determine the number of populations and the maximum time length\n",
    "    num_populations = len(data[0])\n",
    "    max_length = max(max(len(population) for population in sample) for sample in data)\n",
    "    \n",
    "    # Initialize an array to store padded data\n",
    "    padded_data = []\n",
    "    for sample in data:\n",
    "        padded_sample = [np.pad(population, (0, max_length - len(population)), 'constant', constant_values=np.nan) for population in sample]\n",
    "        padded_data.append(padded_sample)\n",
    "    \n",
    "    # Convert the list of padded data to a numpy array for easy mean calculation\n",
    "    padded_data_np = np.array(padded_data)\n",
    "    average_activity = np.nanmean(padded_data_np, axis=0)\n",
    "    \n",
    "    # Calculate valid counts and determine the last valid index based on the trial threshold\n",
    "    valid_counts = np.sum(~np.isnan(padded_data_np), axis=0)\n",
    "    last_valid_indices = np.max(np.where(valid_counts >= trial_threshold, np.arange(valid_counts.shape[1]), 0), axis=1)\n",
    "    min_last_valid_index = np.min(last_valid_indices)\n",
    "    \n",
    "    # Truncate the average activity to the last valid index\n",
    "    truncated_averages = average_activity[:, :min_last_valid_index + 1]\n",
    "    return truncated_averages.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c7e3208",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def average_activity(data, trial_threshold=100):\n",
    "    \"\"\"\n",
    "    Calculate the average activity across data points, truncated to the last valid time point based on a minimum\n",
    "    trial threshold.\n",
    "\n",
    "    Args:\n",
    "        data (list of lists): Data containing time series or trials from multiple entities.\n",
    "        trial_threshold (int): Minimum number of trials required to consider a time point valid.\n",
    "\n",
    "    Returns:\n",
    "        list: Average activity for the data, truncated to the last valid time point where the trial count exceeds the threshold.\n",
    "    \"\"\"\n",
    "    # Find the maximum length across all data points for padding\n",
    "    max_length = max(len(item) for item in data)\n",
    "    \n",
    "    # Pad each data point with NaNs to the maximum length\n",
    "    padded_data = np.array([np.pad(item, (0, max_length - len(item)), 'constant', constant_values=np.nan) for item in data])\n",
    "    \n",
    "    # Calculate the mean, ignoring NaNs for an accurate calculation\n",
    "    average_activity = np.nanmean(padded_data, axis=0)\n",
    "    \n",
    "    # Count the number of non-NaN values at each time point\n",
    "    valid_counts = np.sum(~np.isnan(padded_data), axis=0)\n",
    "    \n",
    "    # Determine the last valid time point based on the trial threshold\n",
    "    last_valid_index = np.max(np.where(valid_counts >= trial_threshold)[0])\n",
    "    \n",
    "    # Truncate the average activity to this last valid index\n",
    "    truncated_average_activity = average_activity[:last_valid_index + 1]\n",
    "    return truncated_average_activity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
